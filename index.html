<html>
  <head>
    <!-- <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css"
            integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous"> -->
    <link
      href="http://fonts.googleapis.com/css?family=Lato:300,400,900"
      rel="stylesheet"
      type="text/css"
    />
    <link href="style.css" rel="stylesheet" />
    <!--fonts google-->
    <link
      href="https://fonts.googleapis.com/css?family=Roboto+Slab:400,700"
      rel="stylesheet"
      type="text/css"
    />
    <link
      href="https://fonts.googleapis.com/css?family=Roboto:400,300,500,700"
      rel="stylesheet"
      type="text/css"
    />

    <script src="./main.js"></script>
    <title>demo</title>
  </head>
  <body>
      <div class="header">
        <h1 class="head">RILA: Reflective and Imaginative Language Agent for Zero-Shot Semantic Audio-Visual Navigation</h1>
        <div class="authors">Anonymous Author(s)</div>
        <div class="intro">
          we propose our Reflective and Imaginative Language Agent (RILA), <br>
          leveraging the inherent commonsense reasoning capabilities of LLMs to perform zero-shot SAVN.
        </div>
         <div class="line"></div>
      </div>
      <div class="bigcontainer">


        <div class="container" id="abstractdiv">
          <div class="guideme">Abstract</div>
          <div style="width:50%; text-align: left; float: left">
            <div class="abstract">
              We leverage Large Language Models (LLM) for zero-
              shot Semantic Audio Visual Navigation (SAVN). Existing
              methods utilize extensive training demonstrations for rein-
              forcement learning, yet achieve relatively low success rates
              and lack generalizability. The intermittent nature of au-
              ditory signals further poses additional obstacles to infer-
              ring the goal information. 
            </div>
            <div class="abstract">
              To address this challenge, we
              present the Reflective and Imaginative Language Agent
              (RILA). By employing multi-modal models to process sen-
              sory data, we instruct an LLM-based planner to actively ex-
              plore the environment. During the exploration, our agent
              adaptively evaluates and dismisses inaccurate perceptual
              descriptions. Additionally, we introduce an auxiliary LLM-
              based assistant to enhance global environmental compre-
              hension by mapping room layouts and providing strategic
              insights. Through comprehensive experiments and analy-
              sis, we show that our method outperforms relevant base-
              lines without training demonstrations from the environment
              and complementary semantic information
            </div>
            <!-- <div class="abstract">
              Through comprehensive experiments and analysis, we show that our
              method outperforms relevant baselines without training
              demonstrations from the environment and complementary semantic
              information. Furthermore, we provide a comprehensive analysis of
              diverse regimes under the soundspace task.
            </div> -->
          </div>
          <img
            class="image"
            style="
              width: 48%;
              margin-top: 0%;
              margin-bottom: 2%;
              height: auto;
              margin-right: 2%;
              text-align: left;
            "
            src="./images/demonstration.jpg"
          ></img>
        </div>
          <div style="height: 5%;background-color: white;"></div>


        <div class="container">
          <div class="guideme">Demo Videos</div>
          <div style="width:20%; margin-left:10%; margin-top:3%; float: left">
            <div class="video-abstract">
              · The goal object is a counter.
            </div>
            <div class="video-abstract">
              · Every step agent will hear 1 second different audio comes from goal object.
            </div>
            <div class="video-abstract">
              · Sound stop at step 14 in this example episode.
            </div>
          </div>
          <video
            src="./images/demo.mp4"
            controls="controls"
            style="
              width: 60%;
              margin-top: 0%;
              margin-bottom: 2%;
              height: auto;
              margin-right: 2%;
              text-align: left;
            "
          ></video>
          </div>
          <div style="height: 5%;background-color: white;"></div>


        <div class="container" >
          <div class="guideme">Method</div>
          <div style="width:33%; margin-left:5% ;margin-top: 2%;text-align: left; float: left">
            <div class="method-abstract">
              We partition our pipeline into different modules:
            </div>
            <div class="method-abstract">
              &nbsp;&nbsp;&nbsp;&nbsp;- Perception<br>
              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- Audio: Direction & Distance<br>
              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- Visual: Semantic map & Object Detection
            </div>
            <div class="method-abstract">
              &nbsp;&nbsp;&nbsp;&nbsp;- Planning<br>
              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- Planner<br>
              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- Assistant: Region Imagination & Suggestion
            </div>
            <div class="method-abstract">
              More details are shown in our paper.
            </div>
          </div>
          <img
            class="image"
            style="
              width: 55%;
              margin-top: 0%;
              margin-bottom: 2%;
              height: auto;
              margin-right: 2%;
              text-align: left;
            "
            src="./images/structure.jpg"
          ></img>
        </div>
          <div style="height: 5%;background-color: white;"></div>


        <div class="container">
          <div class="guideme">Planner Prompting Example</div>
          <div style="width:20%; margin-left:5% ;margin-top: 2%;text-align: left; float: left">
            <div class="prompt-abstract">
              Here is an example of how Planner and Assistant works (simplified):
            </div>
            <div class="prompt-abstract">
              Assistant: Facilitate commonsense in LLM to infer the type of a region and imagine its size. Then give Planner its suggestion
            </div>
            <div class="prompt-abstract">
              Planner: According to perceptual information and Assistant suggestion, make choices about frontiers.
            </div>
            <div class="prompt-abstract">
              More details are shown in our paper.
            </div>
          </div>
          <img class="image" src="./images/example-prompt-final.png" style="
              width: 70%;
              margin-top: 0%;
              margin-bottom: 2%;
              height: auto;
              margin-right: 2%;
              text-align: left;
            "></img>
          </div>
          <div style="height: 5%;background-color: white;"></div>
       <div class="container">
          <div class="guideme">Experiment & Metrix</div>
          <img class="image" src="./images/metric.png" style="
              width: 70%;
              margin-top: 0%;
              margin-bottom: 2%;
              height: auto;
              margin-right: 2%;
              text-align: left;
            "></img>
          <!-- <table
            style="border: 2px #000000 dashed; width: 50%"
            align="center"
            border="2"
          >
            <tbody>
              <tr>
                <td>&nbsp;</td>
                <td><b>Success</b></td>
                <td><b>SPL</b></td>
                <td><b>SNA</b></td>
                <td><b>DTG</b></td>
                <td><b>SWS</b></td>
              </tr>
              <tr>
                <td><b>AV-WAN(2020)</b></td>
                <td>17.2</td>
                <td>13.2</td>
                <td>12.7</td>
                <td>11.0</td>
                <td>6.9</td>
              </tr>
              <tr>
                <td><b>SAVi(2021)</b></td>
                <td>24.8</td>
                <td>17.2</td>
                <td>13.2</td>
                <td>9.9</td>
                <td>14.7</td>
              </tr>
              <tr>
                <td><b>Ours</b></td>
                <td>25.7</td>
                <td>14.5</td>
                <td>8.8</td>
                <td>19.4</td>
                <td>10.8</td>
              </tr>
              <tr>
                <td><b>K-SAVEN(2022)</b></td>
                <td>34.4</td>
                <td>23.4</td>
                <td>21.7</td>
                <td>6.6</td>
                <td>14.3</td>
              </tr>
              <tr>
                <td><b>+ oracle instruction</b></td>
                <td>&nbsp;</td>
                <td>&nbsp;</td>
                <td>&nbsp;</td>
                <td>&nbsp;</td>
                <td>&nbsp;</td>
              </tr>
              <tr>
                <td><b>AVLEN(2022)</b></td>
                <td>26.2</td>
                <td>17.6</td>
                <td>14.2</td>
                <td>9.2</td>
                <td>15.8</td>
              </tr>
              <tr>
                <td><b>CAVEN(2023)</b></td>
                <td>42.0</td>
                <td>30.0</td>
                <td>26.5</td>
                <td>7.6</td>
                <td>30.9</td>
              </tr>
            </tbody>
          </table> -->
        </div>
<div style="height: 5%;background-color: white;"></div>
        </div>
      </div>
    </div>
  </body>
</html>
